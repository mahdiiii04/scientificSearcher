{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importation du Dataset\n",
    "\n",
    "Nous utilisons le jeu de données arXiv, contenant les métadonnées de plus de 2,7 millions d’articles scientifiques au format JSON.  \n",
    "Chaque entrée contient des informations telles que : l’identifiant, le titre, les auteurs, le résumé, les catégories, et les références du papier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "path = '/kaggle/input/arxiv/arxiv-metadata-oai-snapshot.json'\n",
    "\n",
    "data = []\n",
    "with open(path, 'r') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))  # Each line is a separate JSON object\n",
    "\n",
    "print(\"Number of entries:\", len(data))\n",
    "print(\"First paper title:\", data[0]['title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Echantillon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data[14434]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fusion du Titre avec l’Abstrait\n",
    "\n",
    "Pour enrichir la représentation sémantique de chaque papier, nous avons fusionné le **titre** et l’**abstrait** en une seule chaîne de texte.  \n",
    "Cela permet de mieux capturer le contenu global du document, car ces deux éléments contiennent l’essentiel de l’information descriptive sur le papier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def clean(text):\n",
    "    return ' '.join(text.strip().split())\n",
    "\n",
    "\n",
    "papers = pd.DataFrame([{\n",
    "    'id': paper['id'],\n",
    "    'text': clean(paper['title']) + ' ' + clean(paper['abstract'])\n",
    "} for paper in data if paper.get('title') and paper.get('abstract')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "papers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prétraitement des Données\n",
    "\n",
    "Avant d'utiliser les textes dans notre application, nous avons effectué plusieurs étapes de prétraitement pour nettoyer et standardiser les données. Ces étapes sont les suivantes :\n",
    "\n",
    "1. **Mise en minuscule** : pour uniformiser le texte.  \n",
    "2. **Suppression des expressions LaTeX** : élimination des éléments comme `\\textbf{}`, `\\n`, etc., fréquents dans les papiers scientifiques.  \n",
    "3. **Normalisation des espaces** : suppression des espaces superflus.  \n",
    "4. **Suppression des caractères non alphanumériques** (sauf ponctuation commune).  \n",
    "\n",
    "À la fin de cette étape, chaque article est représenté par un **texte nettoyé**, accompagné de son **identifiant (ID)**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_data(text):\n",
    "\n",
    "    #lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    #remove LaTeX formatting\n",
    "    text = re.sub(r'\\\\[a-zA-Z]+', '', text)\n",
    "\n",
    "    #Normalise whitespaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    #Remove non-alphanumeric characters execpt for common ponctuation\n",
    "    text = re.sub(r'[^a-z0-9.,;:!?\\'\"()\\- ]', '', text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "papers['text'] = papers['text'].apply(preprocess_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(papers.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sauvegarde du resultat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "papers.to_parquet(\"papers.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Téléchargement du Modèle Sentence Transformers\n",
    "\n",
    "Pour encoder les textes de manière sémantique, nous utilisons le modèle `all-MiniLM-L6-v2` de la bibliothèque **SentenceTransformers**.  \n",
    "Ce modèle est léger, rapide, et offre de bonnes performances pour les tâches de similarité de texte.\n",
    "\n",
    "### Installation de la bibliothèque\n",
    "\n",
    "```bash\n",
    "pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install sentence_transformers hf_xet > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T15:39:49.738670Z",
     "iopub.status.busy": "2025-05-04T15:39:49.738098Z",
     "iopub.status.idle": "2025-05-04T15:40:01.016211Z",
     "shell.execute_reply": "2025-05-04T15:40:01.015526Z",
     "shell.execute_reply.started": "2025-05-04T15:39:49.738645Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-04 15:39:54.838939: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746373194.864996     194 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746373194.872109     194 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Téléchargement des Résultats Prétraités\n",
    "\n",
    "Le prétraitement des textes a été effectué en amont sur un TPU pour bénéficier d’une plus grande capacité de traitement.  \n",
    "De même, la génération des embeddings a été réalisée sur GPU pour accélérer le calcul."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T22:52:01.898020Z",
     "iopub.status.busy": "2025-05-04T22:52:01.897308Z",
     "iopub.status.idle": "2025-05-04T22:52:13.966368Z",
     "shell.execute_reply": "2025-05-04T22:52:13.965661Z",
     "shell.execute_reply.started": "2025-05-04T22:52:01.897996Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
      "  warnings.warn(\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1-BuspKbP0168ys51JBhrrQfmj2nKf2A2\n",
      "From (redirected): https://drive.google.com/uc?id=1-BuspKbP0168ys51JBhrrQfmj2nKf2A2&confirm=t&uuid=8ac5dfdf-3969-4dd2-b073-f4bdb0f9a568\n",
      "To: /kaggle/working/papers.parquet\n",
      "100%|███████████████████████████████████████| 1.55G/1.55G [00:06<00:00, 243MB/s]\n"
     ]
    }
   ],
   "source": [
    "file_id = '1-BuspKbP0168ys51JBhrrQfmj2nKf2A2'\n",
    "file_name = 'papers.parquet'\n",
    "\n",
    "# Download from Drive\n",
    "!gdown --id {file_id} -O {file_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T22:43:38.543525Z",
     "iopub.status.busy": "2025-05-04T22:43:38.543205Z",
     "iopub.status.idle": "2025-05-04T22:43:53.000928Z",
     "shell.execute_reply": "2025-05-04T22:43:53.000380Z",
     "shell.execute_reply.started": "2025-05-04T22:43:38.543501Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0704.0001</td>\n",
       "      <td>calculation of prompt diphoton production cros...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0704.0002</td>\n",
       "      <td>sparsity-certifying graph decompositions we de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0704.0003</td>\n",
       "      <td>the evolution of the earth-moon system based o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0704.0004</td>\n",
       "      <td>a determinant of stirling cycle numbers counts...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0704.0005</td>\n",
       "      <td>from dyadic  to  in this paper we show how to ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                               text\n",
       "0  0704.0001  calculation of prompt diphoton production cros...\n",
       "1  0704.0002  sparsity-certifying graph decompositions we de...\n",
       "2  0704.0003  the evolution of the earth-moon system based o...\n",
       "3  0704.0004  a determinant of stirling cycle numbers counts...\n",
       "4  0704.0005  from dyadic  to  in this paper we show how to ..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "papers = pd.read_parquet('papers.parquet')\n",
    "\n",
    "papers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Génération des Embeddings avec le Modèle\n",
    "\n",
    "Une fois le modèle `all-MiniLM-L6-v2` chargé, nous pouvons générer les embeddings des textes nettoyés.  \n",
    "Chaque texte (titre + résumé) sera transformé en un vecteur dense représentant sa signification sémantique.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T15:40:14.367023Z",
     "iopub.status.busy": "2025-05-04T15:40:14.366758Z",
     "iopub.status.idle": "2025-05-04T17:34:53.572176Z",
     "shell.execute_reply": "2025-05-04T17:34:53.571186Z",
     "shell.execute_reply.started": "2025-05-04T15:40:14.366993Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fc2a2fabe334fc38b1b525373f05d7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/21255 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embeddings = model.encode(\n",
    "    papers['text'].tolist(),\n",
    "    batch_size=128,\n",
    "    show_progress_bar=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T17:37:26.869616Z",
     "iopub.status.busy": "2025-05-04T17:37:26.869082Z",
     "iopub.status.idle": "2025-05-04T17:37:26.874378Z",
     "shell.execute_reply": "2025-05-04T17:37:26.873817Z",
     "shell.execute_reply.started": "2025-05-04T17:37:26.869593Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2720631, 384)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation de FAISS\n",
    "\n",
    "FAISS (**Facebook AI Similarity Search**) est une bibliothèque conçue pour effectuer des recherches rapides de similarité entre vecteurs à grande échelle.  \n",
    "Elle est particulièrement adaptée à notre cas, où l’on compare des millions d’embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T17:40:06.190090Z",
     "iopub.status.busy": "2025-05-04T17:40:06.189694Z",
     "iopub.status.idle": "2025-05-04T17:40:11.110488Z",
     "shell.execute_reply": "2025-05-04T17:40:11.109678Z",
     "shell.execute_reply.started": "2025-05-04T17:40:06.190062Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (2.4.1)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<3.0,>=1.25.0->faiss-cpu) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<3.0,>=1.25.0->faiss-cpu) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2024.2.0)\n",
      "Downloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl (31.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.11.0\n"
     ]
    }
   ],
   "source": [
    "!pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Génération de l'Index FAISS avec IndexFlatL2\n",
    "\n",
    "Pour effectuer la recherche de similarité, nous utilisons **FAISS** avec la méthode `IndexFlatL2`,  \n",
    "qui repose sur la distance euclidienne (L2) pour comparer les vecteurs.\n",
    "\n",
    "Cette méthode est simple et exacte, bien qu’un peu plus lente que les techniques approximatives,  \n",
    "mais elle reste suffisante pour des projets de taille moyenne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T17:41:17.576499Z",
     "iopub.status.busy": "2025-05-04T17:41:17.575923Z",
     "iopub.status.idle": "2025-05-04T17:41:21.495672Z",
     "shell.execute_reply": "2025-05-04T17:41:21.494853Z",
     "shell.execute_reply.started": "2025-05-04T17:41:17.576475Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "dim = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dim)\n",
    "index.add(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sauvegarde de l'Index FAISS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T17:43:37.062131Z",
     "iopub.status.busy": "2025-05-04T17:43:37.061465Z",
     "iopub.status.idle": "2025-05-04T17:43:50.205646Z",
     "shell.execute_reply": "2025-05-04T17:43:50.204430Z",
     "shell.execute_reply.started": "2025-05-04T17:43:37.062106Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "faiss.write_index(index, \"faiss_index.idx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sauvegarde du Mapping des Identifiants\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T22:44:26.379730Z",
     "iopub.status.busy": "2025-05-04T22:44:26.379375Z",
     "iopub.status.idle": "2025-05-04T22:44:26.794867Z",
     "shell.execute_reply": "2025-05-04T22:44:26.794283Z",
     "shell.execute_reply.started": "2025-05-04T22:44:26.379708Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "papers[['id']].to_parquet(\"id_mapping.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test de Recherche avec l'Index FAISS\n",
    "\n",
    "Maintenant que nous avons sauvegardé notre index et notre mapping des identifiants,  \n",
    "nous pouvons tester la recherche en utilisant une requête pour obtenir les articles les plus similaires.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T18:52:11.523842Z",
     "iopub.status.busy": "2025-05-04T18:52:11.523008Z",
     "iopub.status.idle": "2025-05-04T18:52:11.555740Z",
     "shell.execute_reply": "2025-05-04T18:52:11.555010Z",
     "shell.execute_reply.started": "2025-05-04T18:52:11.523816Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ce506f47ce6472a9828958e5e0a81c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"How to extract triplets from text\"\n",
    "\n",
    "query_emb = model.encode([prompt]).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T18:27:57.697076Z",
     "iopub.status.busy": "2025-05-04T18:27:57.696763Z",
     "iopub.status.idle": "2025-05-04T18:27:58.136808Z",
     "shell.execute_reply": "2025-05-04T18:27:58.135992Z",
     "shell.execute_reply.started": "2025-05-04T18:27:57.697055Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "distances, indices = index.search(query_emb, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T18:27:59.856652Z",
     "iopub.status.busy": "2025-05-04T18:27:59.855882Z",
     "iopub.status.idle": "2025-05-04T18:27:59.862364Z",
     "shell.execute_reply": "2025-05-04T18:27:59.861464Z",
     "shell.execute_reply.started": "2025-05-04T18:27:59.856626Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper ID: 2205.05270\n",
      "Title: relational triple extraction: one step is enough extracting relational triples from unstructured text is an essential task in natural language processing and knowledge graph construction. existing approaches usually contain two fundamental steps: (1) finding the boundary positions of head and tail entities; (2) concatenating specific tokens to form triples. however, nearly all previous methods suffer from the problem of error accumulation, i.e., the boundary recognition error of each entity in step (1) will be accumulated into the final combined triples. to solve the problem, in this paper, we introduce a fresh perspective to revisit the triple extraction task, and propose a simple but effective model, named directrel. specifically, the proposed model first generates candidate entities through enumerating token sequences in a sentence, and then transforms the triple extraction task into a linking problem on a \"head  tail\" bipartite graph. by doing so, all triples can be directly extracted in only one step. extensive experimental results on two widely used datasets demonstrate that the proposed model performs better than the state-of-the-art baselines.\n",
      "--------------------------------------------------\n",
      "Paper ID: 2310.18463\n",
      "Title: benchingmaking large langage models in biomedical triple extraction biomedical triple extraction systems aim to automatically extract biomedical entities and relations between entities. the exploration of applying large language models (llm) to triple extraction is still relatively unexplored. in this work, we mainly focus on sentence-level biomedical triple extraction. furthermore, the absence of a high-quality biomedical triple extraction dataset impedes the progress in developing robust triple extraction systems. to address these challenges, initially, we compare the performance of various large language models. additionally, we present git, an expert-annotated biomedical triple extraction dataset that covers a wider range of relation types.\n",
      "--------------------------------------------------\n",
      "Paper ID: 2111.10692\n",
      "Title: textbook to triples: creating knowledge graph in the form of triples from ai textbook a knowledge graph is an essential and trending technology with great applications in entity recognition, search, or question answering. there are a plethora of methods in natural language processing for performing the task of named entity recognition; however, there are very few methods that could provide triples for a domain-specific text. in this paper, an effort has been made towards developing a system that could convert the text from a given textbook into triples that can be used to visualize as a knowledge graph and use for further applications. the initial assessment and evaluation gave promising results with an f1 score of 82.\n",
      "--------------------------------------------------\n",
      "Paper ID: 1909.01807\n",
      "Title: icdm 2019 knowledge graph contest: team uwa we present an overview of our triple extraction system for the icdm 2019 knowledge graph contest. our system uses a pipeline-based approach to extract a set of triples from a given document. it offers a simple and effective solution to the challenge of knowledge graph construction from domain-specific text. it also provides the facility to visualise useful information about each triple such as the degree, betweenness, structured relation type(s), and named entity types.\n",
      "--------------------------------------------------\n",
      "Paper ID: 2206.01442\n",
      "Title: plumber: a modular framework to create information extraction pipelines information extraction (ie) tasks are commonly studied topics in various domains of research. hence, the community continuously produces multiple techniques, solutions, and tools to perform such tasks. however, running those tools and integrating them within existing infrastructure requires time, expertise, and resources. one pertinent task here is triples extraction and linking, where structured triples are extracted from a text and aligned to an existing knowledge graph (kg). in this paper, we present plumber, the first framework that allows users to manually and automatically create suitable ie pipelines from a community-created pool of tools to perform triple extraction and alignment on unstructured text. our approach provides an interactive medium to alter the pipelines and perform ie tasks. a short video to show the working of the framework for different use-cases is available online under: https:www.youtube.comwatch?vxc9rjniuv8g\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "top_papers = papers.iloc[indices[0]]\n",
    "\n",
    "# Display the top matching papers\n",
    "for i, row in top_papers.iterrows():\n",
    "    print(f\"Paper ID: {row['id']}\")\n",
    "    print(f\"Title: {row['text']}\")\n",
    "    print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 612177,
     "sourceId": 11666150,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31013,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
